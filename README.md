# üåï Mercury 
![Badge Linguagem](https://img.shields.io/badge/Linguagem-Python-blue)
![Badge Framework](https://img.shields.io/badge/biblioteca-Pandas-yellow)
![Badge DB](https://img.shields.io/badge/DB-PostgreSQL-blue)
![Badge API](https://img.shields.io/badge/API-Spotipy-green)
![Badge Status](https://img.shields.io/badge/Status-Development-blue)
### Projeto de ETL utilizando a API do Spotify
Projeto desenvolvido para praticar aprendizados do curso **Design e Modelagem de Data Warehouses** da Data Science Academy.

## Descri√ß√£o do Projeto:
O Mercury tem como objetivo obter insigths atrav√©s dos dados disponibilizados pela API do Spotify. Inicialmente a proposta √© obter o m√°ximo de informa√ß√£o poss√≠vel associada √†s m√∫sicas escutas diariamente por um usu√°rio, e obter seus pr√≥prios insigths.

## Tecnologias e Ferramentas utilizadas:

- ETL: [Python](https://www.python.org/) [(Pandas)](https://pandas.pydata.org/)
- Data Visualization: [Metabase](https://www.metabase.com/)
- Banco de dados: [PostgreSQL](https://www.postgresql.org/)
- Infraestrutura: [Docker](https://www.docker.com/)

## ETL:
1. Extra√ß√£o de dados em lotes consumindo a API do Spotify, obtendo dados referentes √†s ultimas m√∫sicas escutadas em determinado per√≠odo, a partir desse retorno s√£o obtidos os ids de: track, album, artista e o timestamp da escuta.
2. A partir dos ids, s√£o filtrados apenas os ids √∫nicos, para reduzir chamadas aos endpoints na etapa 4.
3. Dependendo do volume de dados, √© necess√°rio separar em lotes, pois a API possui limita√ß√µes de ids por chamada.
4. Com os ids √∫nicos, realizam-se consultas nos respectivos endpoints para obter os dados relacionados aos artistas, tracks e album.
5. Os dados iniciais da primeira extra√ß√£o correspondem a tabela fato, as consultas derivadas da etapa anterior correspondem a dimens√µes, todos os dataframes s√£o gravados na stage para tratamento.
6. Dentro da stage, s√£o executadas stored procedures que tratam os dados, lidam com inconsist√™ncias da fonte e padronizam para serem gravados (upsert) dentro do Data Warehouse.
7. Ap√≥s o processo os dados se encontram limpos, organizados e estruturados no Data Warehouse dispon√≠vel para utiliza√ß√£o no Metabase

## Modelagem de dados:

- O projeto segue o modelo star schema, com granularidade por hora, com uma tabela fato centralizada e diversas dimens√µes relacionadas aos fatos. Essa modelagem de dados permite consultas eficientes e an√°lises detalhadas sobre as m√∫sicas escutadas ao longo do tempo. O star schema e a granularidade por hora ajudam a otimizar as consultas e facilitam a explora√ß√£o dos dados no Metabase ou outras ferramentas de an√°lise. 

<div style="text-align: center;">
  <img src="docs/MercuryERD.png" alt="Modelo Dimensional">
</div>
